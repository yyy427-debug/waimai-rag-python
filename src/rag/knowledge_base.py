import chromadb
import os
from typing import List, Dict, Optional
from src.rag.langchain_utils import get_embeddings  # æ–°å¢LangChainåµŒå…¥å¯¼å…¥

current_script_dir = os.path.dirname(os.path.abspath(__file__))
MERCHANT_FILE_PATH = os.path.join(current_script_dir, "..", "knowledge_base", "merchants.txt")

print(f"ğŸ“Œ å®é™…å•†æˆ·æ–‡ä»¶è·¯å¾„ï¼š{MERCHANT_FILE_PATH}")

class KnowledgeBase:
    def __init__(self, persist_dir: str = "./chroma_db"):
        self.EMBED_MODEL = "nomic-embed-text:latest"
        self.COLLECTION_NAME = "merchant_db"
        self.PERSIST_DIR = persist_dir

        self.client = chromadb.PersistentClient(path=self.PERSIST_DIR)
        self.collection = self.client.get_or_create_collection(
            name=self.COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"}
        )
        print(f"âœ… Chroma é›†åˆ '{self.COLLECTION_NAME}' åˆå§‹åŒ–å®Œæˆ")

    def _extract_tags(self, description: str) -> Dict[str, str]:
        tags = {
            "taste": "", "scene": "", "price": "",
            "æ‹›ç‰Œ": "", "é…é€": "", "ä¼˜æƒ ": ""
        }
        if "ã€å£å‘³ï¼š" in description:
            start = description.find("ã€å£å‘³ï¼š") + 5
            end = description.find("ã€‘", start)
            tags["taste"] = description[start:end].strip() if end != -1 else ""
        if "ã€åœºæ™¯ï¼š" in description:
            start = description.find("ã€åœºæ™¯ï¼š") + 5
            end = description.find("ã€‘", start)
            tags["scene"] = description[start:end].strip() if end != -1 else ""
        price_keywords = ["10å…ƒ", "15å…ƒ", "20å…ƒ", "å¹³ä»·", "é«˜æ€§ä»·æ¯”", "èµ·é€ä»·"]
        for kw in price_keywords:
            if kw in description:
                tags["price"] += kw + "|"
        tags["price"] = tags["price"].rstrip("|")

        if "ã€æ‹›ç‰Œï¼š" in description:
            start = description.find("ã€æ‹›ç‰Œï¼š") + 5
            end = description.find("ã€‘", start)
            tags["æ‹›ç‰Œ"] = description[start:end].strip() if end != -1 else ""
        if "ã€é…é€ï¼š" in description:
            start = description.find("ã€é…é€ï¼š") + 5
            end = description.find("ã€‘", start)
            tags["é…é€"] = description[start:end].strip() if end != -1 else ""
        if "ã€ä¼˜æƒ ï¼š" in description:
            start = description.find("ã€ä¼˜æƒ ï¼š") + 5
            end = description.find("ã€‘", start)
            tags["ä¼˜æƒ "] = description[start:end].strip() if end != -1 else ""

        return tags

    def _extract_item_tags(self, item_part: str) -> str:
        if item_part.startswith("ã€") and item_part.endswith("ã€‘"):
            return item_part[1:-1].strip()
        return ""

    def _get_embedding(self, text: str) -> Optional[List[float]]:
        """æ›¿æ¢ä¸ºLangChainçš„OllamaåµŒå…¥ç”Ÿæˆï¼ˆå…¼å®¹åŸæœ‰é‡è¯•é€»è¾‘ï¼‰"""
        max_retry = 2
        for retry in range(max_retry):
            try:
                # ä½¿ç”¨LangChainå°è£…çš„åµŒå…¥æ¨¡å‹
                embedding = get_embeddings().embed_query(text)
                if len(embedding) >= 100:
                    return embedding
                print(f"âš ï¸  ç¬¬ {retry+1} æ¬¡ç”ŸæˆåµŒå…¥å¤±è´¥ï¼šå‘é‡æ— æ•ˆ")
            except Exception as e:
                print(f"âš ï¸  ç¬¬ {retry+1} æ¬¡ç”ŸæˆåµŒå…¥æŠ¥é”™ï¼š{str(e)}")
        return None

    def load_data(self, file_path: str = MERCHANT_FILE_PATH):
        if not os.path.exists(file_path):
            print(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
            return

        print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ•°æ®ï¼š{file_path}")
        documents, metadatas, embeddings_list, ids = [], [], [], []

        with open(file_path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f.readlines(), 1):
                line = line.strip()
                if not line:
                    continue

                parts = line.split("|")
                if len(parts) < 5:
                    print(f"âŒ è·³è¿‡æ ¼å¼é”™è¯¯è¡Œï¼ˆ{line_num}ï¼‰ï¼š{line}")
                    continue

                merchant_id = parts[0].strip()
                name = parts[1].strip()
                category = parts[2].strip()
                rating = parts[3].strip()
                item_part = parts[4].strip()
                description = "|".join(parts[5:]).strip()

                item_tags = self._extract_item_tags(item_part)
                tags = self._extract_tags(description)

                embed_text = (
                    f"å®ç‰©å…³é”®è¯ï¼š{item_tags} | å®ç‰©å…³é”®è¯ï¼š{item_tags} | å®ç‰©å…³é”®è¯ï¼š{item_tags} | "
                    f"å•†æˆ·åç§°ï¼š{name} | å£å‘³ï¼š{tags['taste']} | åœºæ™¯ï¼š{tags['scene']} | è¯„åˆ†ï¼š{rating}"
                )

                embedding = self._get_embedding(embed_text)
                if not embedding:
                    print(f"âŒ è·³è¿‡å•†æˆ·ï¼ˆ{name}ï¼‰ï¼šåµŒå…¥ç”Ÿæˆå¤±è´¥")
                    continue

                metadatas.append({
                    "merchant_id": merchant_id,
                    "name": name,
                    "category": category,
                    "rating": float(rating) if rating.replace(".", "").isdigit() else 0,
                    "item_tags": item_tags,
                    "taste": tags["taste"],
                    "scene": tags["scene"],
                    "price": tags["price"],
                    "æ‹›ç‰Œ": tags["æ‹›ç‰Œ"],
                    "é…é€": tags["é…é€"],
                    "ä¼˜æƒ ": tags["ä¼˜æƒ "],
                    "raw": line
                })
                print(f"ğŸ“¥ å­˜å‚¨å•†æˆ·ï¼š{name} | æ‹›ç‰Œï¼š{tags['æ‹›ç‰Œ']} | é…é€ï¼š{tags['é…é€']} | ä¼˜æƒ ï¼š{tags['ä¼˜æƒ ']}")
                documents.append(embed_text)
                embeddings_list.append(embedding)
                ids.append(f"merchant_{merchant_id}")

        if documents:
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                embeddings=embeddings_list,
                ids=ids
            )
            print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} æ¡å•†æˆ·æ•°æ®ï¼ˆå«å®ç‰©æ ‡ç­¾+æ‹›ç‰Œ+é…é€+ä¼˜æƒ ï¼‰")
        else:
            print(f"âŒ æ— æœ‰æ•ˆå•†æˆ·æ•°æ®åŠ è½½")

    def search(self, query_text: str, top_k: int = 15) -> Dict[str, List]:
        """åŸæœ‰æ£€ç´¢æ–¹æ³•ï¼ˆä¿ç•™ï¼Œå…¼å®¹æ—§é€»è¾‘ï¼‰"""
        print(f"ğŸ” å‘é‡æ£€ç´¢ï¼šæŸ¥è¯¢='{query_text}'")
        query_embedding = self._get_embedding(query_text)
        if not query_embedding:
            print("âŒ æ£€ç´¢å¤±è´¥ï¼šæŸ¥è¯¢åµŒå…¥ç”Ÿæˆå¤±è´¥")
            return {"documents": [], "metadatas": [], "distances": []}
        try:
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where=None,
                include=["documents", "metadatas", "distances"]
            )
            return {
                "documents": results["documents"][0] if results["documents"] else [],
                "metadatas": results["metadatas"][0] if results["metadatas"] else [],
                "distances": results["distances"][0] if results["distances"] else []
            }
        except Exception as e:
            print(f"âŒ æ£€ç´¢æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
            return {"documents": [], "metadatas": [], "distances": []}

    def get_langchain_retriever(self, top_k: int = 15):
        """æ–°å¢ï¼šè·å–LangChainå…¼å®¹çš„æ£€ç´¢å™¨"""
        from langchain_chroma import Chroma
        langchain_chroma = Chroma(
            client=self.client,
            collection_name=self.COLLECTION_NAME,
            embedding_function=get_embeddings()
        )
        return langchain_chroma.as_retriever(search_kwargs={"k": top_k})

    def search_with_retriever(self, query_text: str, top_k: int = 15) -> Dict[str, List]:
        """æ–°å¢ï¼šé€šè¿‡LangChainæ£€ç´¢å™¨æ£€ç´¢ï¼ˆå…¼å®¹åŸæœ‰è¿”å›æ ¼å¼ï¼‰"""
        try:
            retriever = self.get_langchain_retriever(top_k=top_k)
            docs = retriever.invoke(query_text)
            metadatas = [doc.metadata for doc in docs]
            documents = [doc.page_content for doc in docs]
            return {
                "documents": documents,
                "metadatas": metadatas,
                "distances": [0.0]*len(docs)  # å…œåº•å…¼å®¹
            }
        except Exception as e:
            print(f"âŒ LangChainæ£€ç´¢å¤±è´¥ï¼š{str(e)}")
            return {"documents": [], "metadatas": [], "distances": []}

# åˆå§‹åŒ–å¹¶åŠ è½½æ•°æ®
kb = KnowledgeBase()
kb.load_data()